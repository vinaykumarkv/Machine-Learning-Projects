{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a011f71-9269-4be5-8ecc-511ac3065f34",
   "metadata": {},
   "source": [
    "# certifciate_quality_classifier_v3\n",
    "## This version we will try to add more and more features but still we will keep binary outcome. and we will use scipy and related package to directly call functions related to logistic regression thanks to below contributors we are able to easily calculate and run multiple ML algorithms\n",
    "---\n",
    "*\"Scipy was created by ***Travis Oliphant, Eric Jones, and Pearu Peterson*** in 2001 as part of the effort to create a complete scientific computing environment in Python. This environment is known as the SciPy stack, and includes NumPy, matplotlib, and pandas.\"*\n",
    "\n",
    "***Also learning to do beautiful formatting as above***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c982ab-c952-479b-85b7-8779abaa9fdf",
   "metadata": {},
   "source": [
    "I have downloaded a dataset related to Wine quality lets try to convert that data into something more relevant to our study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6916c1a9-b4cc-4e4b-8937-77b843734a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "#Let me convert Wine data set into Certificate data set for our study\n",
    "#Importing our regular pandas to get the csv loaded\n",
    "\n",
    "import pandas as pd\n",
    "wine_df = pd.read_csv('../ref_datasets/winequality-red.csv',sep=';')\n",
    "print(wine_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348f4ca-97ac-4671-839f-834ca4d8187f",
   "metadata": {},
   "source": [
    "#### oh nice! we have wine quality data, so still its quality data. so let me not try to convert it. I wil start directly with this data set, but still we need to convert Quality column into binary decision, lets calculate mean to identify a simple boundary and convert to 1 or 0 as decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6facf301-9549-4f59-9b1c-cbbb3426f1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6360225140712945\n"
     ]
    }
   ],
   "source": [
    "print(wine_df['quality'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fabaf-0c3d-496b-bd9c-fdc295e759a2",
   "metadata": {},
   "source": [
    "#### So since mean is around 5.5, lets keep that as our boundary/threshold and apply a logic as below to have 1 if greater than 5.5 and 0 if less than 5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40558ba7-ed98-49a6-ae9f-9c5937aae42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  decision  \n",
      "0      9.4        5         0  \n",
      "1      9.8        5         0  \n",
      "2      9.8        5         0  \n",
      "3      9.8        6         1  \n",
      "4      9.4        5         0  \n"
     ]
    }
   ],
   "source": [
    "#importing numpy to apply the logic and create another column to store the decision\n",
    "import numpy as np\n",
    "wine_df['decision'] = np.where(wine_df['quality'] > 5.5, 1, 0)\n",
    "print(wine_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b3e6e-2244-4ff8-9be4-fd0ae7e3b8fa",
   "metadata": {},
   "source": [
    "#### Now lets import ML packages and start with ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a159523a-9e6a-4c65-a92a-481d7d659dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vinay\\AppData\\Local\\Temp\\ipykernel_18292\\2526607318.py:60: OptimizeWarning: Unknown solver options: maxiter\n",
      "  result = minimize(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy Optimization Success: False\n",
      "Optimal Logistic Regression Parameters (Theta): [[ 0.00153738]\n",
      " [ 0.00103492]\n",
      " [-0.00559284]\n",
      " [ 0.00074438]\n",
      " [ 0.00048332]\n",
      " [-0.00227964]\n",
      " [ 0.00113169]\n",
      " [-0.00477754]\n",
      " [-0.00313699]\n",
      " [ 0.00150512]\n",
      " [ 0.00476019]\n",
      " [ 0.00951559]]\n"
     ]
    }
   ],
   "source": [
    "#importing ML packages\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize #we will use this to find parameters to minimize cost function we studied in v2\n",
    "from sklearn.model_selection import train_test_split #we will use this to create training and test data set\n",
    "from sklearn.preprocessing import StandardScaler #this we will use to standardize the feature data\n",
    "\n",
    "# lets prepare the data (x = features and y is target)\n",
    "# lets drop the original 'quality' column as it's correlated with 'decision' /\n",
    "# otherwise it will become major driver in the logic, lets study how other features impact\n",
    "X = wine_df.drop(['quality', 'decision'], axis=1).values\n",
    "y = wine_df['decision'].values.reshape(-1, 1) # Reshape for consistency, which means it will be column vector m x 1 matrix\n",
    "\n",
    "# split the training data and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "#now we will standardize the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) #fits scalar to training data and transforms - which means it will make mean = 0 and standard deviation = 1\n",
    "X_test_scaled = scaler.transform(X_test) #transforms data using mean and standard deviation found in training data\n",
    "# above standardization is necessary to keep same scaling for all features so that optimization happens quickly\n",
    "\n",
    "X_train_biased = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]) # this will add 1 row of 1's for all features just like theta 0\n",
    "\n",
    "# cost function defination\n",
    "\n",
    "def cost_function(theta, X, y):\n",
    "    \"\"\"\n",
    "    calculates cost function with theta, X and y\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    h = 1 / (1 + np.exp(-X @ theta)) # Sigmoid function (h(x)) as we studied yesterday\n",
    "    \n",
    "    # Logistic Regression Cost\n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "# gradient function defination\n",
    "def gradient_function(theta, X, y):\n",
    "    m = len(y)\n",
    "    # Reshape theta to be a column vector\n",
    "    theta_col = theta.reshape(-1, 1) \n",
    "    \n",
    "    # Calculate hypothesis (h)\n",
    "    h = 1 / (1 + np.exp(-X @ theta_col)) \n",
    "    \n",
    "    # Calculate the error vector\n",
    "    error = h - y \n",
    "    \n",
    "    # Calculate the gradient: (1/m) * X_transpose @ error\n",
    "    # Note: np.dot(X.T, error) gives the matrix multiplication\n",
    "    gradient = (1/m) * np.dot(X.T, error)\n",
    "    \n",
    "    # Minimize expects the output to be a 1D array, so we flatten it\n",
    "    return gradient.flatten()\n",
    "# model fitting - initialize parameters\n",
    "initial_theta = np.zeros((X_train_biased.shape[1])) \n",
    "\n",
    "# run the optimization using scipy's minimize\n",
    "result = minimize(\n",
    "    fun=cost_function,\n",
    "    x0=initial_theta,\n",
    "    args=(X_train_biased, y_train),\n",
    "    method='TNC', # Truncated Newton Conjugate Gradient I will explain more in medium blog on this.\n",
    "    #I had used L-BFGS-B method earlier which didnt optimize, I will explain about this in my medium blog - l = limited memory, BFGS = Broyden–Fletcher–Goldfarb–Shanno, B= Bounds,  this speeds up convergence compared to simple gradient decent\n",
    "    jac=gradient_function,\n",
    "    options={'maxiter': 1000} #Sets a limit on the number of optimization steps.\n",
    ")\n",
    "\n",
    "# extract optimized parameters\n",
    "optimal_theta = result.x.reshape(-1, 1) \n",
    "\n",
    "print(\"SciPy Optimization Success:\", result.success)\n",
    "print(\"Optimal Logistic Regression Parameters (Theta):\", optimal_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8bd51e-3ed4-44b5-a05f-f9355542e7fa",
   "metadata": {},
   "source": [
    "#### Now we found optimal paramters for each feature, which contains 11 thetas related to actual features and 1 related to bias feature.\n",
    "#### let me use these optimal paramters to find the prediction and also gather accuracy of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f9249df-ab87-4b78-b778-dda6152ecfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PERFORMANCE EVALUATION\n",
      "Accuracy Score: 0.7229\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Reject (0)       0.66      0.76      0.71       213\n",
      " Approve (1)       0.78      0.69      0.74       267\n",
      "\n",
      "    accuracy                           0.72       480\n",
      "   macro avg       0.72      0.73      0.72       480\n",
      "weighted avg       0.73      0.72      0.72       480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[162  51]\n",
      " [ 82 185]]\n",
      "MODEL INTERPRETATION\n",
      "Model Intercept (Bias): 0.0015\n",
      "Feature Coefficients\n",
      "  alcohol             : +0.0095\n",
      "  volatile acidity    : -0.0056\n",
      "  total sulfur dioxide: -0.0048\n",
      "  sulphates           : +0.0048\n",
      "  density             : -0.0031\n",
      "  chlorides           : -0.0023\n",
      "  pH                  : +0.0015\n",
      "  free sulfur dioxide : +0.0011\n",
      "  fixed acidity       : +0.0010\n",
      "  citric acid         : +0.0007\n",
      "  residual sugar      : +0.0005\n"
     ]
    }
   ],
   "source": [
    "#lets import sklearn metrics to find accuracy, classification report and confusion matrix, remember our first v1\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# again prepare data\n",
    "# add a bias term (column of 1s) to the scaled test data\n",
    "X_test_biased = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled])\n",
    "\n",
    "# make predictions\n",
    "# update optimal_theta to ensure it's a 2D column vector (p x 1) for matrix multiplication, which mean we are 12 rows and 1 column matrix\n",
    "theta_col = optimal_theta.reshape(-1, 1) \n",
    "\n",
    "# Calculate the linear hypothesis (z (h(x) = X * theta))\n",
    "z_test = X_test_biased @ theta_col\n",
    "\n",
    "# Calculate the predicted probabilities using the Sigmoid function or logistic regression\n",
    "probabilities = 1 / (1 + np.exp(-z_test))\n",
    "\n",
    "# Convert probabilities to binary predictions (1 if probability >= 0.5, 0 otherwise)\n",
    "y_pred = (probabilities >= 0.5).astype(int)\n",
    "\n",
    "#lets evaluate the model\n",
    "print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")\n",
    "\n",
    "# Print a detailed Classification Report\n",
    "print(\"Classification Report:\")\n",
    "# y_test must be flattened to match the shape of y_pred for scikit-learn metrics\n",
    "print(classification_report(y_test.flatten(), y_pred.flatten(), target_names=['Reject (0)', 'Approve (1)']))\n",
    "\n",
    "# Print the Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Interpretting model co-efficients\n",
    "print(\"MODEL INTERPRETATION\")\n",
    "feature_names = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', \n",
    "                 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', \n",
    "                 'density', 'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "# The first coefficient is the intercept (theta_0)\n",
    "intercept = optimal_theta[0][0]\n",
    "\n",
    "# Pair the remaining coefficients with their feature names\n",
    "# We use optimal_theta[1:] because index 0 is the intercept\n",
    "coefficients_data = list(zip(feature_names, optimal_theta[1:].flatten()))\n",
    "\n",
    "print(f\"Model Intercept (Bias): {intercept:.4f}\")\n",
    "print(\"Feature Coefficients\")\n",
    "# Sort coefficients by absolute magnitude for easy interpretation of importance\n",
    "for name, coef in sorted(coefficients_data, key=lambda item: abs(item[1]), reverse=True):\n",
    "    # Print a positive or negative sign for clarity\n",
    "    print(f\"  {name:20}: {coef:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918a770-7171-4522-899e-1cb8d53593b6",
   "metadata": {},
   "source": [
    "### With above results:\n",
    "\n",
    "accuracy = 0.7229 = 72.29%\n",
    "\n",
    "78% of time it predicted correctly for 1's (approved)\n",
    "\n",
    "76% of time it predicted correctly for 0's (rejected)\n",
    "\n",
    "Model is better in predicting approve then reject (impleied with f1 score)\n",
    "\n",
    "macro avg provides that both approve and rejects were balanced and didnt observe any bias\n",
    "\n",
    "### confusion matrix:\n",
    "    \n",
    "[TN, FP]\n",
    "[FN, TP]\n",
    "\n",
    "True Negatives (162): Correct prediction= 162 \n",
    "\n",
    "False Positives (51): Incorrect prediction= 51 \n",
    "\n",
    "False Negatives (82): Incorrect prediction= 82 \n",
    "\n",
    "True Positives (185): Correct prediction= 185 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9aa87-378e-4deb-a50d-e83c0159d044",
   "metadata": {},
   "source": [
    "## We were able to achieve 72 % accuracy with multiple features, we will try to upversion this to improve the accuracy by experimenting with:\n",
    "\n",
    "-Featuring Engineering\n",
    "-\n",
    "\n",
    "-Regularization\n",
    "-\n",
    "\n",
    "-Data Balancing and Threshold Tuning\n",
    "-\n",
    "\n",
    "-Alternative Algorithms such as : Support Vector Machines (SVM), Random Forest or Gradient Boosting\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc287778-0ef7-4ec4-b8f5-87a0bdb563d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce9abb-add0-444b-875e-a03956213411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5cfcf-2e57-467c-a0f7-be3cda486b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
